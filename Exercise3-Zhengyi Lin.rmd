---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
## hint r script: glass, tree_example, random_forest, hockey
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(modelr)
library(gbm)
library(caret)
library(ggmap)
library(maps)
library(mapdata)

dengue = read.csv('dengue.csv', header=TRUE)
greenbuildings = read.csv('greenbuildings.csv', header=TRUE)
CAhousing = read.csv('CAhousing.csv', header=TRUE)

# override the default setting of ggplot
theme_set(theme_minimal())
```
## Problem 1: What Causes What?

1. It's a question of causation, and it's hard to draw definitive conclusions just by looking at the size of police forces. Cities with above-average crime rates may hire more police officers than average cities, which can lead to a false conclusion that police are ineffective at solving crime. On the other hand, having more police means that crime is more easily detected, which may lead some to conclude that crime is higher, when in fact it may be the same as in any other city with fewer police. In short, it is hard to say what impact increased policing has had on crime.

2. Basically, the researchers added an IV variable by using the terror alert system. The high terror alert means that no matter how many crimes are committed in a particular area, the police presence will increase. In their first return, they found that the high terror alert was expected to reduce the number of crimes by about seven. In the second return, which controls subway ridership, high terror alerts are expected to reduce crime by about six.

3. The researchers decided to control subway ridership to ensure that the low crime rates resulting from high terror rates were not just a matter of fewer people going out and walking the streets. The researchers tried to capture the effect of high terror rates on the number of people living in cities.

4. The first column includes a linear model using robust regression with three coefficients. One factor only takes into account the effect of a high terror rate within the First police District, the National Mall. That's because if terrorists were to attack Washington, D.C., they would likely focus on this area. The next factor is the effect of high alert on other police districts in Washington, DC. The third coefficient is the logarithm of subway ridership at noon. Basically, this regression suggests that heightened vigilance (and therefore increased police numbers) mainly reduced crime in the National Mall area, and the effect in the rest of the city was not as profound as in other areas, even if it still reduced crime by a small amount. However, the return still shows strong evidence that more officers can lower crime, and that's because the D.C. police force is likely to add the most officers in Ward 1 during periods of heightened alert.

## Problem 2 Tree Modeling: Dengue Cases
### Part 1: CART

```{r 2, message=FALSE, echo=FALSE}
#fixing na values 
dengue <- na.exclude(dengue)
dengue$city = dengue$city %>% factor()
dengue$season = dengue$season %>% factor()
#create a testing and training set
dengue_split = initial_split(dengue, prop = 0.9)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)
#creating the tree, CART model
dengue_tree = rpart(total_cases ~ ., data = dengue_train,
                    control = rpart.control(cp = 0.002, minsplit=30))
rpart.plot(dengue_tree, digits=-5, type=4, extra=1)
```

The model above shows the un-pruned CART Tree, we will proceed to prune and then calculate RMSE.


``` {r 2 cont , message=FALSE, echo=FALSE}
# this function actually prunes the tree at that level
prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}
#lets prune to make sure we have the best model
prune_dengue_tree = prune_1se(dengue_tree)
#checking
rmse_CART = rmse(prune_dengue_tree, dengue_test)
cat(rmse_CART,' RMSE for Pruned CART Model') 
```

### Part 2: Random Forest
``` {r 2 cont again , message=FALSE, echo=FALSE}
#random forest
DengueRandom = randomForest(total_cases ~ ., data= dengue_train, importance = TRUE)
plot(DengueRandom)
```

This plot shows the out of bag MSE as a function of the number of trees used. Let's proceed to look at the RMSE compared to the testing set.


``` {r 2 random conclusion, message=FALSE, echo=FALSE}
rmse_random = rmse(DengueRandom, dengue_test)
cat(rmse_random,' RMSE for Random Forest')
```

### Part 3: Gradient Boosted Trees

``` {r 2 boosted , message=FALSE, echo=FALSE}
#boosted trees
DengueBoost = gbm(total_cases ~ ., data= dengue_train,
             interaction.depth=4, n.trees=350, shrinkage=.05, cv.folds = 10, 
             distribution='gaussian')
gbm.perf(DengueBoost)
```

This plot shows the error curve of the Gradient Boosted Model, with the optimal number of trees listed as output. Let's now check the RMSE for the Gradient Boosted Trees Model. 


``` {r 2 boosted conclusion, message=FALSE, echo=FALSE}
#checking
rmse_boosted = rmse(DengueBoost, dengue_test) 
cat(rmse_boosted,' RMSE for Gradient Boosted Trees') 
```

Looking at the RMSE results from the three models, it appears that random forest would be the best choice for this particular set of data. The next section shows the partial dependency plots for the Random Forest Model. 

### Part 4: Partial Dependency Plots
``` {r 2 PD plots, message=FALSE, echo=FALSE}
#pd plots
partialPlot(DengueRandom, dengue_test, 'specific_humidity', las=1)
partialPlot(DengueRandom, dengue_test, 'precipitation_amt', las=1)
partialPlot(DengueRandom, dengue_test, 'tdtr_k', las=1)
```

### Wrap Up: 

Looking at the PD plots, most seem to make sense in the context of the science of mosquito breeding. Mosquitos require standing water in order to make baby mosquitos, it makes sense that as precipitation increases, the number of mosquitos increases, the increased number of mosquitos leads to more cases of Dengue. The same seems to be true of humidity. Humidity is a measure of how much evaporated moisture there is in the air, higher humidity would seem to indicate that there is a higher amount of water on the ground, and thus the amount of mosquito breeding grounds. Our wild card PD plot looks at the Average Diurnal Temperature Range. It shows that as DTR increases, the amount of predicted Dengue cases decreases. This makes sense as well, it's possible that temperature shocks kill mosquitos which leads to less Dengue cases. 






```{r Q3 - CART, include=FALSE, echo=FALSE}
# create the revenue per per square foot variable 
greenbuildings = mutate(greenbuildings, revenue = Rent * leasing_rate)
greenbuildings = greenbuildings %>% drop_na()
# split data into training and testing
set.seed(100)
green_split =  initial_split(greenbuildings, prop=0.8)
green_train = training(green_split)
green_test  = testing(green_split)
# let's fit a single tree
green.tree = rpart(revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07 - leasing_rate - Rent, data=green_train, control = rpart.control(cp = 0.00001), na.action=na.omit)
```


```{r Q3 - random forest, include=FALSE}
# now a random forest
green.forest = randomForest(revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07- leasing_rate - Rent, data=green_train, na.action=na.omit, importance = TRUE)
```


```{r Q3 - gbm tuning for green, include=FALSE}
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  green.gbm.tune <- gbm(
    revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07- leasing_rate - Rent, 
    data = green_train,
    distribution = "gaussian",
    n.trees = 500,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(green.gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(green.gbm.tune$valid.error))
}
# check which parameters are performing better 
top10_green = hyper_grid %>% 
              arrange(min_RMSE) %>%
              head(10)
# Then use this new grid 2 to run the loop again
# grid 2 
hyper_grid <- expand.grid(
  shrinkage = c(.05, .1, .2),
  interaction.depth = c(12, 15, 17),
  n.minobsinnode = c(3, 5, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,              
  min_RMSE = 0                     
)
# decided on these parameters, and fit the final bgm
green.boost = gbm(revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07- leasing_rate - Rent, data=green_train, interaction.depth=18, n.trees=600, shrinkage=.2, cv.folds = 10)
# compare RMSE 
rmse_green.tree=rmse(green.tree, green_test)
rmse_green.forest=rmse(green.forest, green_test) 
rmse_green.boost=rmse(green.boost, green_test) 
```



## Problem 3: Green Certification
### Introduction
This question asks us to quantify the effect of green certifications on revenue per square foot in buildings with such a certification. Green certifications clearly have an environmental impact, but do they also make buildings more attractive to potential renters, and people pay attention when buidlings recieve a green certification? We attempt to answer these concerns below. 

### Analysis
#### Data Cleaning 
Since we focused on the revenue of the property, we generated a new variable `revenue` as the product of `Rent` and `leasing_rate`. We created green_rating to see the overall impact of green certificate instead of using both  `LEED` and `EnergyStar` (`green_rating` is a collapsed version of these two ratings). Other important factors that will affect energy usage are the cooling degree days and the heating degree days, in our model we used `total_dd_07` to represent this factor.

#### Modeling
We tried three different models to see which one will yield the best out-of-sample RMSE, they are  CART, random forest, and gradient-boosted model. We split the data set into training and testing sets, and we trained all of the three models on training data using all of the variables.

To tune the gradient-boosted model, we first created a grid that specifies `shrinkage`, `interaction.depth`, `n.minobsinnode`, and `bag.fraction`. then ran the gbm  across all different combination of these parameters. After narrowing the tuning grid  2 times, the average RMSE across 10 folds is still higher than random forest. The final comparison are shown below.


#### Model performacne
The out-of-sample RMSE of the models:
CART: 772.38
Random Forest: 609.07
Gradient-boosted: 683.56

So we decided to use random forest as our best predictive model.


#### Plots
This is the variable importance plot for our random forest model. As you can see, size, market rent, and age seem to be the biggest factors in predicting. Our green rating variable is actually show to have the lowest importance out of all of the parameters. 
```{r Q3 - VI plot,fig.width=5,fig.align='center', echo=FALSE}
# variable importance measures
vi = varImpPlot(green.forest, type=1)
```

As you can see, it appears that green rating is only estimated to increase revenue by fifty dollars.
```{r partail plot, echo=FALSE}
# partial dependence plots
# these are trying to isolate the partial effect of specific features
# on the outcome
pdp::partial(green.forest ,pred.var = "green_rating") %>% 
  ggplot() +
    geom_col(aes(x = factor(green_rating), y=yhat, fill = factor(green_rating))) +
    labs(x = "Green Certification",y = "Predicted Value", title = "Partial dependence plot of Green Certification")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
          guides(color = guide_legend(title=" Green Certification"))
```


### Wrap up
After testing three models, we decided to employ random forest. Using our predictive modeling, we found that green certification doesn’t really lead to a dramatic increase in revenue. According to our partial dependence plot, a green certification is only expected to increase yearly rent revenue (per square foot) by fifty dollars. 




```{r Q4 - plot 1 + tree and forest, include=FALSE}
# split data into training and testing:    
set.seed(101)
ca_split =  initial_split(CAhousing, prop=0.8)
ca_train = training(ca_split)
ca_test  = testing(ca_split)
# fit a single tree
ca.tree = rpart(medianHouseValue ~ . , data=ca_train, control = rpart.control(cp = 0.00001))
# random forest 
ca.forest = randomForest(medianHouseValue ~ . , data=ca_train, control = rpart.control(cp = 0.00001), importance=TRUE)
```


```{r Q4 - ca.bgm tuning, include=FALSE}
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = medianHouseValue ~ .,
    data = ca_train,
    distribution = "gaussian",
    n.trees = 700,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}
# check which parameters are performing better 
hyper_grid %>% 
  arrange(min_RMSE) %>%
  head(10)
# narrow the grid: second try
hyper_grid <- expand.grid(
  shrinkage = c(.1, .3, .5),
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)
# final boosted model
ca.boost = gbm(medianHouseValue ~ ., data = ca_train, distribution = "gaussian", interaction.depth=5, n.trees=659,  shrinkage=.3, cv.folds =10)
```




```{r Q4 compare three models, echo=FALSE, message=FALSE}
# the model we choose here is: random forest for now 
rmse_ca.tree = rmse(ca.tree, ca_test)
rmse_ca.forest = rmse(ca.forest, ca_test)
rmse_ca.boost = rmse(ca.boost, ca_test)
```



```{r Q4 - plots, include=FALSE}
# getting the California data
states <- map_data("state")
ca_df <- subset(states, region == "california")
# plain map of ca
ca_base <- ggplot(data = ca_df, mapping = aes(x = long, y = lat)) + 
  coord_fixed(1.3) + 
  geom_polygon(color = "black", fill = "gray")
# PLOT1: original data
ca_plot1 <- ca_base + geom_point(data = CAhousing, aes(x=longitude, y=latitude,    color=medianHouseValue))+scale_color_continuous(type = "viridis")+
          labs(title = " Actual Median House Value in California", x="longitude", y="latitude")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
          guides(color = guide_legend(title="Median Value"))
# PLOT2: prediction 
CAhousing = CAhousing %>%
  mutate(ca_pred = predict(ca.boost, CAhousing))
ca_plot2 <- ca_base + geom_point(data = CAhousing, aes(x=longitude, y=latitude, color=ca_pred)) + 
            scale_color_continuous(type = "viridis")+
            labs(title = "Predicted Median House Value in California", x="longitude", y="latitude")+
            theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
            guides(color = guide_legend(title=" Predicted Value"))
# PLOT3: residual
CAhousing = CAhousing %>%
  mutate(ca_resid = sqrt((medianHouseValue-ca_pred)^2))
ca_plot3 <- ca_base +
            geom_point(data = CAhousing, aes(x=longitude, y=latitude, color=ca_resid)) + 
            scale_color_continuous(type = "viridis")+
            labs(title = "Residuals of Median House Value in California", x="longitude", y="latitude")+
            theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
            guides(color = guide_legend(title="Residual"))
```

## Problem 4: California Housing
#### Modeling
Again in this question we will compare CART, random forest, and gbm to get the best predictive model.
After tuning the gbm model, we get a out-of-sample RMSE that is smaller than random forest.
So the best predictive model we will use here would be gbm.

Now, we use the gbm model to produce a plot of prediction and a plot of model's residuals.

#### Plots
To produce the three plots, first we set up the base plot of California using the data from the package `maps`.

The plot displays the median house value in California with the colors becoming brighter as the house value increases. The plot clearly displays the higher home values of the Los Angeles and San Francisco Bay Area (including coastal suburbs), having the most concentrated collection of homes with high values. 

```{r Q4 - plot 1, echo=FALSE}
ca_plot1
```
This plot displays the predicted median house value from our model. As you can see, compared to the actual data, our model appears to do a good job at predicting house value. 

```{r Q4 - plot 2, echo=FALSE}
ca_plot2
```
This plot shows the absolute value of the residuals between actual and predicted values. It appears that most of the errors from our model are small. 

```{r Q4 - plot 3,echo = FALSE}
ca_plot3
```
